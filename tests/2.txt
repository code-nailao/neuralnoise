「推理革命」爆发100天：DeepSeek-R1复现研究全揭秘！
图片


  新智元报道  

编辑：犀牛
【新智元导读】本文深入梳理了围绕DeepSeek-R1展开的多项复现研究，系统解析了监督微调（SFT）、强化学习（RL）以及奖励机制、数据构建等关键技术细节。

最近，推理语言模型（RLMs）已经成为主流。

最新发布的、性能最强的LLM大都是推理模型。

尤其是DeepSeek-R1的发布，更是引发了广泛的社会影响，同时也点燃了研究社区对推理的热情。

但是，DeepSeek-R1的一些实现细节还没有完全开源，比如DeepSeek-R1-Zero以及蒸馏的小模型等。

因此，许多复制DeepSeek-R1的研究应运而生（图1），试图通过相似的训练流程和完全开源的训练数据来重现DeepSeek-R1的优异性能。

图片
这些研究探索了监督微调（SFT）和基于可验证奖励的强化学习（RLVR）的可行策略，重点关注数据准备和方法设计，产出了不少宝贵经验。

为此，本文总结了近期的这些复现研究，以启发未来的探索。

图片
论文地址：https://arxiv.org/abs/2505.00551

本文的结构大致对应DeepSeek-R1的训练流程，介绍当前在SFT、RLVR以及其他增强推理能力技术方面的复制工作：

监督微调提升语言模型推理能力：研究团队全面梳理了通过监督微调（Supervised Fine-tuning, SFT）增强语言模型推理能力的相关研究。

用可验证奖励强化学习训练推理语言模型：研究团队介绍了近期通过可验证奖励强化学习（Reinforcement Learning from Verifiable Rewards, RLVR）训练RLMs的研究，详细阐述了训练数据、学习算法和奖励系统设计。

推理语言模型的更多发展方向：研究团队注意到，尽管DeepSeek-R1推动了RLMs的训练，但仍有许多监督策略尚未探索。他们提出了RLMs的更多发展方向，包括奖励建模和偏好优化，并分析了当前RLMs的优缺点，例如强大的分布外泛化能力和偶尔的过度思考。


图片
通过监督微调提升RLMs

推理数据集大多数从收集多样化领域的问题开始，例如数学、科学、编程和谜题，数据来源包括现有的基准测试或网络爬取。

在收集原始数据后，通常会进行多轮过滤以提升数据质量，包括：

去重：通过嵌入相似性或n-gram方法去除重复数据；

拒绝采样：剔除低质量数据；

真值验证：确保数据准确性。

为了保证数据的覆盖面和丰富性，许多数据集在选择过程中明确强调难度和多样性，通常使用启发式方法或模型通过率来优先选择较难的问题。

此外，大多数数据集依赖经过验证的思维链（COTs）或解决方案来确保正确性和质量。

验证方法因领域而异，例如：

数学问题通常通过Math Verify验证；

编程问题通过代码执行或单元测试验证；

通用任务则由大语言模型（LLM）作为评判者进行验证。

这种结合领域验证和选择性保留的方法，使数据管理人员能够提炼出高质量的推理轨迹，从而更好地支持监督微调。

虽然这些数据集覆盖多个领域，但如表1所示，大多数数据集主要集中在数学和编程任务上。涉及更广泛推理任务（如科学、逻辑谜题和开放性问题）的覆盖率仍然相对有限。

图片
值得注意的例外包括DeepSeek-R1和AM，它们在数据收集和蒸馏过程中纳入了更广泛的领域，旨在培养更通用的推理能力。

图片

长度分布

图2展示了数据集的token长度分布情况。

尽管这些数据集的长思维链（CoTs）都来源于同一个教师模型――DeepSeek-R1，但它们的分布却存在明显差异。

例如，AM和Synthetic-1的数据集倾向于较短的序列，而Light-R1和Open-R1的分布范围更广，尾部更长，这表明它们包含更多复杂问题，这些问题通常会引发更长的思维链。

图片
图3中展示了常用数学推理数据集之间的交叉引用结构。该图清晰地呈现了数据集之间的依赖网络和共享数据，帮助研究人员更好地解读结果，避免重复的训练或评估设置。

图片
图中箭头从源数据集指向包含其部分数据的目标数据集。以淡紫色高亮显示的数据集包含从DeepSeek-R1提取的思维链（Chain-of-Thought）轨迹

图片

性能比较

在实践中，SFT阶段对于让基础模型从更强的模型中学习高质量推理轨迹至关重要。

表2展示了在常见数学推理基准（如AIME24/25和MATH500）上的SFT结果比较，突出不同数据集选择和初始模型检查点的影响。

图片
虽然许多方法强调通过增加训练样本数量来提升性能，但LIMO和S1k-1.1表明，通过精心挑选的小规模数据集也能取得优异成果。

图片

训练细节

对于复杂推理等长上下文任务，通常会调整模型配置中的RoPE缩放因子（θ）和最大上下文长度，以支持扩展的上下文能力。

例如，Open-R1将θ设为300,000，上下文长度设为32,768个token。常用的学习率包括1.0 × 10??和5.0 × 10??，批大小通常为96或128。

此外，通常采用打包（packing）技术来提高训练效率。


图片
RLVR在推理语言模型中的应用

图片

RL数据集

DeepSeek-R1-Zero通过独立的RLVR流程在推理和知识任务中取得了优异表现。其RLVR过程中使用的高质量精选数据集是成功的关键。

因此，多项复制研究探索了如何利用开源数据和强大模型高效创建训练数据集的策略。

这些数据集涵盖R训练中可验证的多种任务，主要聚焦于数学和编程问题解决的数据集。表3提供了这些数据集的统计概览。

图片
图片

RL组件

随着DeepSeek-R1-Zero和DeepSeek-R1的发布，DeepSeek展示了通过强化学习（RL）微调LLM以应对复杂推理任务的成功经验。

基于精心挑选的训练数据，相关研究主要集中在配置RL框架的关键部分，以实现卓越性能：采用高效的RL算法（如GRPO）以及设计奖励机制。

表4提供了这些研究方法的比较。

图片
表4总结了多个竞争性开源 DeepSeek-R1 复制研究在强化学习验证任务（RLVR）中使用的算法和奖励设计方案。为了便于比较，DeepSeek-R1 系列模型的相关信息被单独列出

在基于结果-奖励的RL方法中，PPO和GRPO是最常用的微调大语言模型的算法。

有趣的是，近期的一些复制研究对这些方法进行了各种改进，针对特定目标优化了训练效果。

研究团队回顾了几种代表性的基于RL的大语言模型微调算法，包括 REINFORCE、PPO、GRPO及其变体。此外，他们还梳理了这些方法的改进及其背后的动机，旨在清晰概述基于结果-奖励的RL训练方法的技术进步。

图片

奖励机制

奖励是RL训练的核心，因为它定义了优化的目标，引导模型的行为。

一个设计良好的奖励机制能提供清晰、一致的信号，帮助模型学习到有效的策略。

然而，奖励模型常常容易被「奖励欺骗」（reward hacking，指模型通过钻空子获得高分而非真正解决问题），因此近期研究更倾向于使用基于规则的结果奖励系统。

这些系统通常分为三类：

准确性奖励：准确性奖励评估回答是否正确，通常给正确回答打 1 分，错误回答打 0 分或 -1 分。

格式奖励：格式奖励鼓励回答遵循预定义的结构或推理格式，通常给正确格式打 1 分，偏离格式则打 0 分或 -1 分。

长度奖励：长度奖励影响模型回答的详尽程度。一些方法奖励生成特定长度的回答，而另一些方法则鼓励在保证准确性的前提下尽量简洁。

图片

采样策略

直观来说，在训练过程中合理选择样本对RL的有效性至关重要。

一方面，课程学习方法通过逐步增加任务难度，提高了复杂样本的利用率。另一方面，合理使用拒绝采样技术可以提升样本效率并稳定训练。

图片

RLVR在其他任务上的应用

通过RLVR，DeepSeek-R1的复杂推理能力显著增强，在复杂语境理解和问题解决等推理密集型任务中取得成功。

RLVR使大模型能够在无需人工指导的情况下，通过可验证的答案学习和执行任务，激发其复杂推理能力。

受此启发，多项研究探索了RLVR在不同任务中的复杂推理范式。

逻辑推理：TinyZero和Mini-R1尝试在倒计时游戏中重现DeepSeek R1的「灵光一现」时刻，使用简单的基于规则的奖励系统。

面向应用的实际任务：推理语言模型需要通过思考、规划和反思来学习处理现实世界的应用型任务。

超越监督的探索：通过强化学习过程，研究发现大模型展现出了令人惊喜且意想不到的能力。

这些结果凸显了复杂推理语言模型通过RL训练策略，超越监督数据资源甚至人类能力的潜力。


图片
更多发展方向

虽然DeepSeek-R1的成功推进了RLMs的训练，但仍有许多监督策略有待探索。

推理增强的替代方法 :旨在解决传统 RLVR 在捕捉中间步骤和对齐人类期望方面的局限性。

主要方向包括：

过程级奖励建模 (Process-level Reward Modeling, PRM)：对推理的中间步骤提供反馈，而非仅评估最终结果。例如rStar-Math使用过程偏好模型和自我演进，PRIME使用隐式PRM，仅依赖结果标签进行训练，更具可扩展性并减少奖励欺骗。

偏好优化策略 (Preference Optimization)：特别是 直接偏好优化 (Direct Preference Optimization, DPO)，相比PPO或GRPO计算资源需求更少。一些研究探索使用DPO提升推理能力，如Light-R1、Iterative DPO、RedStar、DPO-R1。

泛化性:RLMs在学习推理能力时，能够很好地泛化到域外任务。

持续预训练（例如在数学领域）能显著增强专业和通用推理能力。

监督微调 (SFT) 通过提供高质量示例和结构化归纳先验，对泛化能力至关重要，为后续强化学习奠定稳定基础。精心策划的高质量数据尤为重要。

强化学习 (RL) 展示了强大的域外泛化潜力，甚至超越了模仿学习。经过RL训练的模型可以在不同任务、语言和模态上泛化，例如Llama3-SWE-RL和RL-Poet。像AGRO这样整合On-policy和Off-policy经验的方法可以增强泛化能力。

安全性 :推理语言模型面临一些安全挑战，包括过度思考（生成过长推理链，增加成本，可能忽略环境反馈） 和奖励欺骗（模型利用奖励函数的漏洞或模糊性获取高分）。

自我演进过程引入了失控和未对齐的风险。

越狱攻击 (Jailbreaking) 是一个普遍关注的问题。推理增强的模型可能会牺牲安全性（「安全税」）。

应对措施包括改进算法设计、训练策略、对齐安全策略以及开发具有推理能力的防护模型。

多模态和多语言:

多模态推理语言模型：整合视觉、音频等多种模态。当前多模态模型的推理能力通常弱于单模态模型。将单模态推理能力迁移到多模态是前景广阔但具有挑战性的方向。

多语言推理语言模型：主要挑战在于某些语言资源的有限性。在英语中训练的推理能力向其他语言泛化程度有限。可能需要专门的能力来促进跨语言的洞察或「顿悟」。未来的研究需要专注于更高效的跨语言训练策略，特别是针对低资源语言。


图片
结论

在本文中，研究团队全面概述了受DeepSeek-R1启发而进行的复现工作，特别重点关注了其背后的监督微调和强化学习方法。

他们探讨了开源项目如何整理指令微调数据集，如何实现基于结果奖励的强化学习策略，以及如何设计旨在增强模型推理能力的奖励系统。

除了总结当前各项工作的趋势之外，还对该领域未来充满希望的方向提出了自己的看法。这些方向包括将推理技能扩展到数学和编程任务之外，提升模型的安全性和可解释性，以及改进奖励机制以促进更复杂的推理行为。

团队希望本次综述不仅能捕捉到近期进展，还能为正在进行的研究提供坚实的基础，并标志着向实现通用人工智能迈出了更进一步。












带你一文读懂DeepSeek-R1新模型，为何震动了全球AI圈


图片

本文关注DeepSeek-R1在技术上最重要的突破――用纯深度学习的方法让AI自发涌现出推理能力。这一研究可能会对模型推理训练后续的范式产生深刻影响。
时隔不到一个月，DeepSeek又一次震动全球AI圈。

去年 12 月，DeepSeek推出的DeepSeek-V3在全球AI领域掀起了巨大的波澜，它以极低的训练成本，实现了与GPT-4o和Claude Sonnet 3.5等顶尖模型相媲美的性能。

和上次不同的是，这次推出的新模型DeepSeek-R1不仅成本低，更是在技术上有了大幅提升。而且，它还是一个开源模型。

这款新模型延续了其高性价比的优势，仅用十分之一的成本就达到了GPT-o1级别的表现。所以，很多业内人士甚至喊出了“DeepSeek接班OpenAI”的口号。

比如，前Meta AI工作人员、知名AI论文推特作者Elvis就强调，DeepSeek-R1的论文堪称瑰宝，因为它探索了提升大语言模型推理能力的多种方法，并发现了其中更明确的涌现特性。

图片
另一位AI圈大V Yuchen Jin则认为，DeepSeek-R1论文中提出的，模型利用纯RL方法引导其自主学习和反思推理这一发现，意义非常重大。

图片
英伟达GEAR Lab项目负责人Jim Fan在推特中也提到了，DeepSeek-R1用通过硬编码规则计算出的真实奖励，而避免使用任何 RL 容易破解的学习奖励模型。这使得模型产生了自我反思与探索行为的涌现。

Jim Fan 甚至认为，它们做了OpenAI本来应该做的事，开源。

图片
那么问题来了，他们所提到的纯RL方法训练模型是指什么？

模型出现的“Aha Moment”，又凭什么能证明AI具有了涌现能力？

我们更想知道的是，DeepSeek-R1的这一重要创新对于AI领域未来的发展，究竟意味着什么？



1、用最简单的配方，回归最纯粹的强化学习
在o1推出之后，推理强化成了业界最关注的方法。

一般来说，一个模型在训练过程中只会尝试一种固定训练方法来提升推理能力。

而DeepSeek团队在R1的训练过程中，直接一次性实验了三种截然不同的技术路径：直接强化学习训练（R1-Zero）、多阶段渐进训练（R1）和模型蒸馏，还都成功了。多阶段渐进训练方法和模型蒸馏都包含着很多创新意义元素，对行业有着重要影响。

其中最让人激动的，还是直接强化学习这个路径。因为DeepSeek-R1是首个证明这一方法有效的模型。

我们先来了解一下，训练AI的推理能力传统的方法通常是什么：一般是通过在SFT（监督微调）加入大量的思维链（COT）范例，用例证和复杂的如过程奖励模型（PRM）之类的复杂神经网络奖励模型，来让模型学会用思维链思考。

甚至会加入蒙特卡洛树搜索（MCTS），让模型在多种可能中搜索最好的可能。

图片
传统的模型训练路径
但DeepSeek-R1-Zero选择了一条前所未有的路径“纯”强化学习路径，它完全抛开了预设的思维链模板（Chain of Thought）和监督式微调（SFT），仅依靠简单的奖惩信号来优化模型行为。

这就像让一个天才儿童在没有任何范例和指导的情况下，纯粹通过不断尝试和获得反馈来学习解题。

DeepSeek-R1-Zero 有的只是一套最简单的奖励系统，来激发AI的推理能力。

这个规则就两条：

1.准确性奖励：准确性奖励模型评估响应是否正确。对了就加分，错了扣分。评价方法也很简单：例如，在具有确定性结果的数学问题中，模型需要以指定格式（如和间）提供最终答案；对于编程问题，可以使用编译器根据预定义的测试用例生成反馈。

2.格式奖励：格式奖励模型强制要求模型将其思考过程置于和标签之间。没这么做就扣分，做了就加分。

为了准确观察模型在强化学习（RL）过程中的自然进展，DeepSeek甚至有意将系统提示词仅约束限制在这种结构格式上，来避免任何内容特定的偏见――例如强制让模型进行反思性推理或推广特定的问题解决策略。

图片
R1 Zero的系统提示词
靠着这么一个简单的规则，让AI在GRPO（Group Relative Policy Optimization）的规则下自我采样+比较，自我提升。

GRPO的模式其实比较简单，通过组内样本的相对比较来计算策略梯度，有效降低了训练的不稳定性，同时提高了学习效率。

简单来说，你可以把它想象成老师出题，每道题让模型同时回答多次，然后用上面的奖惩规则给每个答案打分，根据追求高分、避免低分的逻辑更新模型。

这个流程大概是这样的：

输入问题 → 模型生成多个答案 → 规则系统评分 → GRPO计算相对优势 → 更新模型

这种直接训练方法带来了几个显著的优势。首先是训练效率的提升，整个过程可以在更短的时间内完成。其次是资源消耗的降低，由于省去了SFT和复杂的奖惩模型，计算资源的需求大幅减少。

更重要的是，这种方法真的让模型学会了思考，而且是以“顿悟”的方式学会的。



2、用自己的语言，在“顿悟”中学习
我们是怎么看出模型在这种非常“原始”的方法下，是真的学会了“思考”的呢？

论文记录了一个引人注目的案例：在处理一个涉及复杂数学表达式 √a - √(a + x) = x 的问题时，模型突然停下来说"Wait, wait. Wait. That's an aha moment I can flag here"（等等、等等、这是个值得标记的啊哈时刻），随后重新审视了整个解题过程。这种类似人类顿悟的行为完全是自发产生的，而不是预先设定的。

图片
img
这种顿悟往往是模型思维能力跃升的时刻。

因为根据DeepSeek的研究，模型的进步并非均匀渐进的。在强化学习过程中，响应长度会出现突然的显著增长，这些"跳跃点"往往伴随着解题策略的质变。这种模式酷似人类在长期思考后的突然顿悟，暗示着某种深层的认知突破。

图片

在这种伴随着顿悟的能力提升下，R1-Zero在数学界享有盛誉的AIME竞赛中从最初的15.6%正确率一路攀升至71.0%的准确率。而让模型对同一问题进行多次尝试时，准确率甚至达到了86.7%。这不是简单的看过了就会做了――因为AIME的题目需要深度的数学直觉和创造性思维，而不是机械性的公式应用。模型基本必须能推理，才可能有这样的提升。

图片
img
另一个模型确实通过这种方法学会了推理的另一个核心证据，是模型响应长度会根据问题的复杂度自然调节。这种自适应行为表明，它不是在简单地套用模板，而是真正理解了问题的难度，并相应地投入更多的"思考时间"。就像人类面对简单的加法和复杂的积分会自然调整思考时间一样，R1-Zero展现出了类似的智慧。

最有说服力的或许是模型展现出的迁移学习能力。在完全不同的编程竞赛平台Codeforces上，R1-Zero达到了超过96.3%人类选手的水平。这种跨域表现表明，模型不是在死记硬背特定领域的解题技巧，而是掌握了某种普适的推理能力。



3、一个聪明，但口齿不清的天才
尽管R1-Zero展现出了惊人的推理能力，但研究者们很快发现了一个严重的问题：它的思维过程往往难以被人类理解。

论文坦诚地指出，这个纯强化学习训练出来的模型存在"poor readability"（可读性差）和"language mixing"（语言混杂）的问题。

这个现象其实很好理解：R1-Zero完全通过奖惩信号来优化其行为，没有任何人类示范的"标准答案"作为参考。就像一个天才儿童自创了一套解题方法，虽然屡试不爽，但向别人解释时却语无伦次。它在解题过程中可能同时使用多种语言，或者发展出了某种特殊的表达方式，这些都让其推理过程难以被追踪和理解。

正是为了解决这个问题，研究团队开发了改进版本DeepSeek-R1。通过引入更传统的"cold-start data"（冷启动数据）和多阶段训练流程，R1不仅保持了强大的推理能力，还学会了用人类易懂的方式表达思维过程。这就像给那个天才儿童配了一个沟通教练，教会他如何清晰地表达自己的想法。

在这一调教下之后，DeepSeek-R1展现出了与OpenAI o1相当甚至在某些方面更优的性能。在MATH基准测试上，R1达到了77.5%的准确率，与o1的77.3%相近；在更具挑战性的AIME 2024上，R1的准确率达到71.3%，超过了o1的71.0%。在代码领域，R1在Codeforces评测中达到了2441分的水平，高于96.3%的人类参与者。

图片

然而，DeepSeek-R1 Zero的潜力似乎更大。它在AIME 2024测试中使用多数投票机制时达到的86.7%准确率――这个成绩甚至超过了OpenAI的o1-0912。这种"多次尝试会变得更准确"的特征，暗示R1-Zero可能掌握了某种基础的推理框架，而不是简单地记忆解题模式。

论文数据显示，从MATH-500到AIME，再到GSM8K，模型表现出稳定的跨域性能，特别是在需要创造性思维的复杂问题上。这种广谱性能提示R1-Zero可能确实培养出了某种基础的推理能力，这与传统的特定任务优化模型形成鲜明对比。

所以，虽然口齿不清，但也许DeepSeek-R1-Zero才是真正理解了推理的“天才”。



4、纯粹强化学习，也许才是通向AGI的意外捷径
之所以DeepSeek-R1的发布让圈内人的焦点都投向了纯强化学习方法，因为它完全可以说得上是打开了AI 进化的一条新路径。

R1-Zero――这个完全通过强化学习训练出来的AI模型，展现出了令人惊讶的通用推理能力。它不仅在数学竞赛中取得了惊人成绩。

更重要的是，R1-Zero不仅是在模仿思考，而是真正发展出了某种形式的推理能力。

因为在过往的训练方法中，尤其在监督微调中使用训练好的神经网络来评估质量的话，模型可能学会触发奖励模型的特定模式，生成对奖励模型"口味"的内容，而不是真正提升推理能力。换句话说，AI系统找到了获得高奖励但实际上违背训练目标的投机取巧方式。这就是我们常说的奖励欺骗（reward hacking）。但R1-Zero用极简的奖励规则基本避免了奖励欺骗的可能性――规则太简单了，没有什么“口味”可以去模仿。模型在这个情况下发展出的推理能力更可信，也更自然。

这个发现可能会改变我们对机器学习的认识：传统的AI训练方法可能一直在重复一个根本性的错误，我们太专注于让AI模仿人类的思维方式了，业界需要重新思考监督学习在AI发展中的角色。通过纯粹的强化学习，AI系统似乎能够发展出更原生的问题解决能力，而不是被限制在预设的解决方案框架内。

虽然R1-Zero在输出可读性上存在明显缺陷，但这个"缺陷"本身可能恰恰印证了其思维方式的独特性。就像一个天才儿童发明了自己的解题方法，却难以用常规语言解释一样。这提示我们：真正的通用人工智能可能需要完全不同于人类的认知方式。

这才是真正的强化学习。就像著名教育家皮亚杰的理论：真正的理解来自于主动建构，而不是被动接受。










